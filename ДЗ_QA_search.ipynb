{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ по поиску\n",
    "\n",
    "Привет! Вам надо реализивать поисковик на базе вопросов-ответов с сайта [pravoved.ru](https://pravoved.ru/questions-archive/).        \n",
    "Поиск должен работать на трех технологиях:       \n",
    "1. обратном индексе     \n",
    "2. word2vec         \n",
    "3. doc2vec      \n",
    "\n",
    "Вы должны понять, какой метод и при каких условиях эксперимента на этом корпусе работает лучше.          \n",
    "Для измерения качества поиска найдите точность (accuracy) выпадания правильного ответа на конкретный вопрос (в этой базе у каждого вопроса есть только один правильный ответ). Точность нужно измерить для всей базы.    \n",
    "При этом давайте считать, что выпал правильный ответ, если он попал в **топ-5** поисковой выдачи.\n",
    "\n",
    "> Сделайте ваш поиск максимально качественным, чтобы значение точности стремилось к 1.     \n",
    "Для этого можно поэкспериментировать со следующим:       \n",
    "- модель word2vec (можно брать любую из опен сорса или обучить свою)\n",
    "- способ получения вектора документа через word2vec: простое среднее арифметическое или взвешивать каждый вектор в соответствии с его tf-idf      \n",
    "- количество эпох у doc2vec (начинайте от 100)\n",
    "- предобработка документов для обучения doc2vec (удалять / не удалять стоп-слова)\n",
    "- блендинг методов поиска: соединить результаты обратного индекса и w2v, или (что проще) w2v и d2v\n",
    "\n",
    "На это задание отведем 10 дней. Дэдлайн сдачи до полуночи 12.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3.mystem import Mystem\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "import os\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from string import punctuation, digits\n",
    "from math import log\n",
    "\n",
    "punctuation = set(punctuation + '«»—–…“”\\n\\t' + digits)\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "from gensim import matutils\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tqdm import tqdm_notebook\n",
    "from judicial_splitter import splitter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(input_text, del_stopwords=True, del_digit=False):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [mystem.lemmatize(x)[0] for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_files(mystem, file, files_list):\n",
    "    if file.endswith('.txt'):\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    else:\n",
    "        text = file\n",
    "    \n",
    "    table = str.maketrans({ch: ' ' for ch in punctuation})\n",
    "    tokenized = word_tokenize(text.replace('\\ufeff', '').lower().translate(table))\n",
    "    words_list = [mystem.lemmatize(word)[0] for word in tokenized]\n",
    "    text_length = len(tokenized)\n",
    "    \n",
    "    document_length[files_list.index(file)] = text_length\n",
    "    \n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inverted_index(mystem, files_list):\n",
    "    \"\"\"\n",
    "    Create inverted index by input doc collection and count the length of each document \n",
    "    :return: inverted index\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(list)\n",
    "    global document_length\n",
    "    document_length = [None] * len(files_list)\n",
    "\n",
    "    for file in files_list:\n",
    "        for word in preprocess_files(mystem, file, files_list):\n",
    "            inverted_index[word].append(files_list.index(file))\n",
    "            \n",
    "    with open('inverted_index.json', 'w', encoding='utf-8') as fw:\n",
    "        json.dump(inverted_index, fw, ensure_ascii=False)\n",
    "\n",
    "    with open('document_length.json', 'w', encoding='utf-8') as fw:\n",
    "        json.dump(document_length, fw, ensure_ascii=False)\n",
    "    \n",
    "    return inverted_index, document_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_BM25(qf, dl, avgdl, k1, b, N, n):\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    score = log((N - n + 0.5) / (n + 0.5)) * (k1 + 1) * qf / (qf + k1 * (1 - b + b * dl / avgdl))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_sim(lemma, inverted_index, document_length):\n",
    "    \"\"\"\n",
    "    Compute similarity score between word in search query and all document from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    if inverted_index.get(lemma):\n",
    "        doc_list = inverted_index[lemma]\n",
    "        relevance_score = {}\n",
    "        avgdl = sum(document_length) / len(document_length)\n",
    "        N = len(document_length)\n",
    "    \n",
    "        for doc in range(N):    \n",
    "            qf = Counter(inverted_index[lemma])[doc]\n",
    "            relevance_score[doc] = score_BM25(qf, document_length[doc], avgdl,\n",
    "                                          2.0, 0.75, N, len(set(inverted_index[lemma])))\n",
    "        return relevance_score\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_search_result(query, inverted_index, files_list, document_length, num_res):\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    Collect as pair (doc_id, score)\n",
    "    :param query: input text\n",
    "    :return: list of lists with (doc_id, score)\n",
    "    \"\"\"\n",
    "    relevance_dict = defaultdict(float)\n",
    "    \n",
    "    for lemma in query:\n",
    "        score = compute_sim(lemma, inverted_index, document_length)\n",
    "        if score:\n",
    "            for elem in score:\n",
    "                relevance_dict[elem] += score[elem]    \n",
    "    result = sorted(relevance_dict, key=relevance_dict.get, reverse=True)[:num_res]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_vectors(model, lemmas):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    vec_list = []\n",
    "    \n",
    "    for word in lemmas:\n",
    "        try:\n",
    "            vec = model.wv[word]\n",
    "            vec_list.append(vec)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return (sum(vec_list) / len(vec_list))\n",
    "\n",
    "\n",
    "def save_w2v_base(files_list, model, mystem, save=True, title='w2v_base'):\n",
    "    \"\"\"Индексирует всю базу для поиска через word2vec\"\"\"\n",
    "    documents_info = []    \n",
    "    \n",
    "    for i, file in tqdm_notebook(enumerate(files_list)):\n",
    "        if file.endswith('.txt'):\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "        else:\n",
    "            text = file\n",
    "            file = i\n",
    "\n",
    "        lemmas = preprocessing(text)\n",
    "        vec = get_w2v_vectors(model, lemmas)\n",
    "            \n",
    "        file_info = {'file': file, 'word2vec': vec}\n",
    "        documents_info.append(file_info)\n",
    "    \n",
    "    if save:\n",
    "        with open(title + '.pkl', 'wb') as fw:\n",
    "            pickle.dump(documents_info, fw)\n",
    "    \n",
    "    return documents_info\n",
    "\n",
    "\n",
    "def search_w2v(query, w2v_model, data_word2vec, n_results):\n",
    "    vec1 = get_w2v_vectors(w2v_model, query)\n",
    "    similarity_dict = {}\n",
    "    \n",
    "    for elem in data_word2vec:\n",
    "        sim = similarity(vec1, elem['word2vec'])\n",
    "        similarity_dict[sim] = elem['file']\n",
    "        \n",
    "    relevant = [similarity_dict[sim] for sim in sorted(similarity_dict, reverse=True)[:n_results]]\n",
    "    return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_paragraphs(files_list, mystem, del_stopwords=False):\n",
    "    file_text = {}\n",
    "    data = []\n",
    "    \n",
    "    for i, file in enumerate(files_list):\n",
    "        if file.endswith('.txt'):\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                file_text[file] = text\n",
    "        else:\n",
    "            text = file\n",
    "            file = i\n",
    "        \n",
    "        paragraphs = splitter(text, 1)\n",
    "            \n",
    "        for paragraph in paragraphs:\n",
    "            paragraph_lemmatized = preprocessing(paragraph, del_stopwords)\n",
    "            data.append({'file': file, 'paragraph': paragraph_lemmatized})\n",
    "\n",
    "    if file_text:\n",
    "        with open('file_text', 'w', encoding='utf-8') as fw:\n",
    "            json.dump(file_text, fw)\n",
    "        return data, file_text\n",
    "    \n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(data, epochs, save=True, title='d2v_model'):\n",
    "    tagged_data = [TaggedDocument(words=elem['paragraph'],tags=[str(i)]) for i, elem in enumerate(data)]\n",
    "    model = Doc2Vec(vector_size=100, min_count=5, alpha=0.025, min_alpha=0.025, epochs=epochs, workers=4, dm=1)\n",
    "    \n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    if save:\n",
    "        with open(title + '.pkl', 'wb') as fw:\n",
    "            pickle.dump(model, fw)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_d2v_vectors(model, lemmas):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    vec = model.infer_vector(lemmas)\n",
    "    return vec\n",
    "    \n",
    "\n",
    "def save_d2v_base(model, paragraphs, save=True, title='d2v_base'):\n",
    "    \"\"\"Индексирует всю базу для поиска через doc2vec\"\"\"\n",
    "    documents_info = []    \n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        vec = get_d2v_vectors(model, paragraph['paragraph'])\n",
    "            \n",
    "        file_info = {'file': paragraph['file'], 'doc2vec': vec}\n",
    "        documents_info.append(file_info)\n",
    "    \n",
    "    if save:\n",
    "        with open(title + '.pkl', 'wb') as fw:\n",
    "            pickle.dump(documents_info, fw)\n",
    "    \n",
    "    return documents_info \n",
    "\n",
    "\n",
    "def search_d2v(query, d2v_model, data_doc2vec, n_results):\n",
    "    vec1 = get_d2v_vectors(d2v_model, query)\n",
    "    similarity_dict = {}\n",
    "    \n",
    "    for elem in data_doc2vec:\n",
    "        sim = similarity(vec1, elem['doc2vec'])\n",
    "        similarity_dict[sim] = elem['file']\n",
    "        \n",
    "    relevant = [similarity_dict[sim] for sim in sorted(similarity_dict, reverse=True)[:n_results]]\n",
    "    return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('qa_corpus.pkl', 'rb') as file:\n",
    "    qa_corpus = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего в корпусе 1384 пары вопрос-ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1384"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый элемент блока это вопрос, второй - ответ на него"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nДобрый день.Мой сын гражданин Украины (ДНР),имеет вид на жительство в Р.Ф., кот.получил проживая с 2014 г. в Нижегородской области.В 2017г. переехал на постоянное место жительство в г.Ростов.Официально трудоустроился на одно из промышл.предприятий г.Ростова.Оформил временную регистрацию в Ростове.В УФМС предупредили,что по истечении 90 дней он должен либо постоянно прописаться либо покинуть территорию России.Прошу проконсультировать как быть дальше.(Вернуться домой в Донецк,но здесь идет война,работы нет.В Ростове он работает по специальности.Он инженер машиностроитель.)Временная прописка до 15 марта.  Если он сможет приобрести какую либо недвижимость,как долго будет решаться вопрос о его постоянной прописке в Ростове.Как в этом случае будет решаться вопрос с видом на жительство в Ростове? Не получится ли ,что приобретя квартиру,он не успеет в ней прописаться до окончании срока временной регистрации. С уважением Людмила Евгеньевна.\\n',\n",
       " 'Добрый вечер!Из Вашего вопроса вообще ничего не ясно.Ваш сын по ВНЖ в Нижегородской обл. сделал временную\\xa0 на 90 дней в Ростове? Так? Или в чем заключается вопрос?С ув., АлёнаМиграционный юристРостов-на-Дону ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = [elem[0] for elem in qa_corpus]\n",
    "answers = [elem[1] for elem in qa_corpus]\n",
    "\n",
    "with open('questions.json', 'w', encoding='utf-8') as fw:\n",
    "    json.dump(questions, fw)\n",
    "\n",
    "with open('answers.json', 'w', encoding='utf-8') as fw:\n",
    "    json.dump(answers, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('questions.json', 'r', encoding='utf-8') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "with open('answers.json', 'r', encoding='utf-8') as f:\n",
    "    answers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.7 s, sys: 3.48 s, total: 20.2 s\n",
      "Wall time: 45.6 s\n"
     ]
    }
   ],
   "source": [
    "inverted_index, document_length = get_inverted_index(mystem, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('inverted_index.json', 'r', encoding='utf-8') as f:\n",
    "    inverted_index = json.load(f)\n",
    "\n",
    "with open('document_length.json', 'r', encoding='utf-8') as f:\n",
    "    document_length = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(query, search_method, n_results=5, return_answer_text=False):\n",
    "    \n",
    "    query = preprocessing(query, del_stopwords=False)\n",
    "    \n",
    "    try:\n",
    "        if search_method == 'inverted_index':\n",
    "            search_result = get_search_result(query, inverted_index, answers, document_length, n_results)\n",
    "\n",
    "        elif search_method == 'word2vec':\n",
    "            search_result = search_w2v(query, w2v_model, data_word2vec, n_results)\n",
    "\n",
    "        elif search_method == 'doc2vec':\n",
    "            search_result = search_d2v(query, d2v_model, data_doc2vec, n_results)\n",
    "\n",
    "        else:\n",
    "            raise TypeError('unsupported search method')\n",
    "\n",
    "    except:\n",
    "        search_result = ['Не найдено результатов по заданному запросу']\n",
    "    \n",
    "    if not return_answer_text:\n",
    "        return search_result\n",
    "    \n",
    "    results = [(index, answers[index]) for index in search_result]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = FastText.load('/Users/alinashaymardanova/Downloads/araneum_none_fasttextskipgram_300_5_2018/araneum_none_fasttextskipgram_300_5_2018.model')\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638f7631de2946caad102e3e254e4095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 16.6 s, sys: 3.69 s, total: 20.3 s\n",
      "Wall time: 44.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_word2vec = save_w2v_base(answers, w2v_model, mystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3292d01d291e42bba6d173fcbd7789c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0050578034682080926"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score = 0\n",
    "answers_index = []\n",
    "\n",
    "for i, question in enumerate(tqdm_notebook(questions)):\n",
    "\n",
    "    search_result = search(question, 'word2vec')\n",
    "\n",
    "    if i in search_result:\n",
    "        accuracy_score += 1\n",
    "        answers_index.append(i)\n",
    "\n",
    "final_accuracy = accuracy_score / len(questions)        \n",
    "\n",
    "final_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = FastText.load('/Users/alinashaymardanova/Downloads/araneum_none_fasttextskipgram_300_5_2018/araneum_none_fasttextskipgram_300_5_2018.model')\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110bc1c6015a4bab864c7846a42b8b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 16.7 s, sys: 3.7 s, total: 20.4 s\n",
      "Wall time: 43.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_word2vec = save_w2v_base(answers, w2v_model, mystem, title='w2v_base_cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bc92648fe04de4b85974ce43a46b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0050578034682080926"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score = 0\n",
    "answers_index = []\n",
    "\n",
    "for i, question in enumerate(tqdm_notebook(questions)):\n",
    "\n",
    "    search_result = search(question, 'word2vec')\n",
    "\n",
    "    if i in search_result:\n",
    "        accuracy_score += 1\n",
    "        answers_index.append(i)\n",
    "\n",
    "final_accuracy = accuracy_score / len(questions)        \n",
    "\n",
    "final_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec со стоп-словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paragraphs = get_paragraphs(answers, mystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 7s, sys: 3min 26s, total: 14min 33s\n",
      "Wall time: 8min 42s\n"
     ]
    }
   ],
   "source": [
    "d2v_model = train_doc2vec(paragraphs, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_doc2vec = save_d2v_base(d2v_model, paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdb474f6806482a972d3e74086e3872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.002890173410404624"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score = 0\n",
    "answers_index = []\n",
    "\n",
    "for i, question in enumerate(tqdm_notebook(questions)):\n",
    "\n",
    "    search_result = search(question, 'doc2vec')\n",
    "\n",
    "    if i in search_result:\n",
    "        accuracy_score += 1\n",
    "        answers_index.append(i)\n",
    "\n",
    "final_accuracy = accuracy_score / len(questions)        \n",
    "\n",
    "final_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec без стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "without_stopwords = get_paragraphs(answers, mystem, del_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_doc2vec = save_d2v_base(d2v_model, without_stopwords, title='d2v_base_without_stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c6569589354c1a89f52787d0cc4d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.001445086705202312"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score = 0\n",
    "answers_index = []\n",
    "\n",
    "for i, question in enumerate(tqdm_notebook(questions)):\n",
    "\n",
    "    search_result = search(question, 'doc2vec')\n",
    "\n",
    "    if i in search_result:\n",
    "        accuracy_score += 1\n",
    "        answers_index.append(i)\n",
    "\n",
    "final_accuracy = accuracy_score / len(questions)        \n",
    "\n",
    "final_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соединим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merging(w2v, d2v, all_): \n",
    "    ans = {}\n",
    "    for item in all_:\n",
    "        if item in w2v: \n",
    "            it_w = w2v[item]\n",
    "        else: it_w = 0\n",
    "            \n",
    "        if item in d2v: \n",
    "            it_d = d2v[item]\n",
    "        else: it_d = 0\n",
    "        \n",
    "        ans[item] = (it_w * 0.7 + it_d * 0.3) / 2\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_w2v_(query, w2v_model, data_word2vec, n_results):\n",
    "    vec1 = get_w2v_vectors(w2v_model, query)\n",
    "    similarity_dict = {}\n",
    "    \n",
    "    for elem in data_word2vec:\n",
    "        sim = similarity(vec1, elem['word2vec'])\n",
    "        similarity_dict[sim] = elem['file']\n",
    "        \n",
    "    relevant = [similarity_dict[sim] for sim in sorted(similarity_dict, reverse=True)[:n_results]]\n",
    "   \n",
    "    return relevant_w2v\n",
    "\n",
    "def search_d2v_(query, d2v_model, data_doc2vec, n_results):\n",
    "    vec2 = get_d2v_vectors(d2v_model, query)\n",
    "    similarity_dict = {}\n",
    "    \n",
    "    for elem in data_doc2vec:\n",
    "        sim = similarity(vec1, elem['doc2vec'])\n",
    "        similarity_dict[sim] = elem['file']\n",
    "        \n",
    "    relevant = [similarity_dict[sim] for sim in sorted(similarity_dict, reverse=True)[:n_results]]\n",
    "    \n",
    "    return relevant_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_comb(query, w2v_model, d2v_model, n_results=5):\n",
    "    relevant_w2v = search_w2v_(query, w2v_model, data_word2vec, n_results)\n",
    "    relevant_d2v = search_d2v_(query, d2v_model, data_doc2vec, n_results)\n",
    "    all_ = set(relevant_w2v) | set(relevant_d2v)\n",
    "    ans = merging(relevant_w2v, relevant_d2v, all_)\n",
    "\n",
    "    return sorted(ans.items(), reverse=True, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6fe880322849b58532a1374595fabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-05c4b4b6c8fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msearch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_comb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word2vec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc2vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearch_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-d11327a5531e>\u001b[0m in \u001b[0;36msearch_comb\u001b[0;34m(query, w2v_model, d2v_model, n_results)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msearch_comb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrelevant_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_w2v_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_word2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mrelevant_d2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_d2v_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_doc2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mall_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_w2v\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_d2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_d2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-0b3881482ccb>\u001b[0m in \u001b[0;36msearch_w2v_\u001b[0;34m(query, w2v_model, data_word2vec, n_results)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msearch_w2v_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_word2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mvec1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_w2v_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msimilarity_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_word2vec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-3c9254459765>\u001b[0m in \u001b[0;36mget_w2v_vectors\u001b[0;34m(model, lemmas)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mfinal_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "accuracy_score = 0\n",
    "answers_index = []\n",
    "\n",
    "for i, question in enumerate(tqdm_notebook(questions)):\n",
    "    \n",
    "    search_result = search_comb(question, 'word2vec', 'doc2vec')\n",
    "\n",
    "    if i in search_result:\n",
    "        accuracy_score += 1\n",
    "        answers_index.append(i)\n",
    "\n",
    "final_accuracy = accuracy_score / len(questions)        \n",
    "\n",
    "final_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a7471173e4439789a11f4b67c77306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 3h 28min 39s, sys: 1min 24s, total: 3h 30min 3s\n",
      "Wall time: 3h 39min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inv_index_score, inv_index_res = get_accuracy(questions, 'inverted_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4877167630057804"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_index_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
