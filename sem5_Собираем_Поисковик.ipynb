{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 5    \n",
    "## Собираем поисковик \n",
    "\n",
    "![](https://bilimfili.com/wp-content/uploads/2017/06/bir-urune-emek-vermek-o-urune-olan-deger-algimizi-degistirir-mi-bilimfilicom.jpg) \n",
    "\n",
    "\n",
    "Мы уже все знаем, для того чтобы сделать поисковик. Осталось соединить все части вместе.    \n",
    "Итак, для поисковика нам понадобятся:         \n",
    "**1. База документов **\n",
    "> в первом дз - корпус Друзей    \n",
    "в сегодняшнем дз - корпус юридических вопросов-ответов    \n",
    "в итоговом проекте - корпус Авито   \n",
    "\n",
    "**2. Функция индексации**                 \n",
    "Что делает: собирает информацию о корпусе, по которуму будет происходить поиск      \n",
    "Своя для каждого поискового метода:       \n",
    "> A. для обратного индекса она создает обратный индекс (чудо) и сохраняет статистики корпуса, необходимые для Okapi BM25 (средняя длина документа в коллекции, количество доков ... )             \n",
    "> B. для поиска через word2vec эта функция создает вектор для каждого документа в коллекции путем, например, усреднения всех векторов коллекции       \n",
    "> C. для поиска через doc2vec эта функция создает вектор для каждого документа               \n",
    "\n",
    "   Не забывайте сохранить все, что насчитает эта функция. Если это будет происходить налету во время поиска, понятно, что он будет работать сто лет     \n",
    "   \n",
    "**3. Функция поиска**     \n",
    "Можно разделить на две части:\n",
    "1. функция вычисления близости между запросом и документом    \n",
    "> 1. для индекса это Okapi BM25\n",
    "> 2. для w2v и d2v это обычная косинусная близость между векторами          \n",
    "2. ранжирование (или просто сортировка)\n",
    "\n",
    "\n",
    "Время все это реализовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## В предыдущей серии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from string import punctuation, digits\n",
    "from math import log\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from judicial_splitter import splitter\n",
    "from tqdm import tqdm_notebook\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "\n",
    "punctuation = set(punctuation + '«»—–…“”\\n\\t' + digits)\n",
    "mystem = Mystem()\n",
    "\n",
    "\n",
    "def preprocess_words(mystem, text):\n",
    "    table = str.maketrans({ch: ' ' for ch in punctuation})\n",
    "    \n",
    "    tokenized = word_tokenize(text.replace('\\ufeff', '').lower().translate(table))\n",
    "    return [mystem.lemmatize(word)[0] for word in tokenized], len(tokenized)\n",
    "\n",
    "\n",
    "def preprocess_files(mystem, file, files_list):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        words_list, text_length = preprocess_words(mystem, f.read())\n",
    "        document_length[files_list.index(file)] = text_length\n",
    "        \n",
    "        return words_list\n",
    "    \n",
    "def get_inverted_index(mystem, files_list, save=True):\n",
    "\n",
    "    inverted_index = defaultdict(list)\n",
    "    global document_length\n",
    "    document_length = [None] * len(files_list)\n",
    "    for file in files_list:\n",
    "        for word in preprocess_files(mystem, file, files_list):\n",
    "            inverted_index[word].append(files_list.index(file))\n",
    "            \n",
    "    if save:\n",
    "        with open('inverted_index.json', 'w', encoding='utf-8') as fw:\n",
    "            json.dump(inverted_index, fw, ensure_ascii=False)     \n",
    "        with open('document_length.json', 'w', encoding='utf-8') as fw:\n",
    "            json.dump(document_length, fw, ensure_ascii=False)\n",
    "    \n",
    "    return inverted_index, document_length\n",
    "\n",
    "\n",
    "def score_BM25(input_, episod, avgdl, k1, b, N, n, data):\n",
    "    arr = mystem.lemmatize(cleaner(input_))\n",
    "    result = 0 \n",
    "    for word in arr:\n",
    "        if (word in data.index) and (episod in data.columns):\n",
    "            n = df_inv.loc[word]['count'] \n",
    "            f = data.loc[word][episod] / (log((N - n + 0.5) / n + 0.5))\n",
    "            a = (k1 + 1) * f\n",
    "            b = f + k1 * (1 - b + b * (len(arr)) / avgdl)\n",
    "            result += (a / b) * log((N - n + 0.5) / n + 0.5)\n",
    "    \n",
    "\n",
    "\n",
    "def compute_sim(text, doc, data):\n",
    "\n",
    "    relevance_score = score_BM25(text, doc, avgdl, k1, b, N, n, data)\n",
    "    \n",
    "    return relevance_score\n",
    "\n",
    "\n",
    "\n",
    "def get_search_result(query, inverted_index, mystem, files_list, document_length, num_res):\n",
    "\n",
    "    relevance_dict = defaultdict(float)\n",
    "    lemmas, _ = preprocess_words(mystem, query)\n",
    "    \n",
    "    for lemma in lemmas:\n",
    "        score = compute_sim(lemma, inverted_index, document_length)\n",
    "        for elem in score:\n",
    "            relevance_dict[elem] += score[elem]\n",
    "            \n",
    "    result = sorted(relevance_dict, key=relevance_dict.get, reverse=True)[:num_res]\n",
    "    \n",
    "    return [files_list[ind] for ind in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Индексация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "### Задание 1\n",
    "Загрузите любую понравившуюся вам word2vec модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = FastText.load('/Users/alinashaymardanova/Downloads/araneum_none_fasttextskipgram_300_5_2018/araneum_none_fasttextskipgram_300_5_2018.model')\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 \n",
    "Напишите функцию индексации для поиска через word2vec. Она должна для каждого документа из корпуса строить вектор.   \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только  вектор, но и опознователь текста, которому он принадлежит. \n",
    "Для поисковика это может быть url страницы, для поиска по текстовому корпусу сам текст.\n",
    "\n",
    "> В качестве документа для word2vec берите **параграфы** исходного текста, а не весь текст целиком. Так вектора будут более осмысленными. В противном случае можно получить один очень общий вектор, релевантый совершенно разным запросам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(mystem, input_text, del_stopwords=True, del_digit=False):\n",
    "\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    words = [x.lower().strip(string.punctuation + '»«–…—') for x in nltk.word_tokenize(input_text)]\n",
    "    lemmas = [mystem.lemmatize(x)[0] for x in words if x]\n",
    "    \n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        \n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_vectors(model, lemmas):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    \n",
    "    vectors_ = [] \n",
    "    for word in lemmas:\n",
    "        try:\n",
    "            vectors_.append(model.wv[word])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return (sum(vectors_)/len(vectors_))\n",
    "\n",
    "\n",
    "def save_w2v_base(files, model, mystem, save=True):\n",
    "    \"\"\"Индексирует всю базу для поиска через word2vec\"\"\"\n",
    "    \n",
    "    info_ = []    \n",
    "    \n",
    "    for file in tqdm_notebook(files):\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            lemmas = preprocessing(mystem, text)\n",
    "            vec = get_w2v_vectors(model, lemmas)\n",
    "            info_.append({'file': file, 'word2vec': vec})\n",
    "    \n",
    "    if save:\n",
    "        with open('w2v_base.pkl', 'wb') as fw:\n",
    "            pickle.dump(info_, fw)\n",
    "    \n",
    "    return info_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_files = '/Users/alinashaymardanova/Downloads/article/'\n",
    "files = list(map(lambda x: path_to_files + x, os.listdir(path_to_files)[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alinashaymardanova/Downloads/article/220648.txt'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbd2cc1277848378ede3d735f22bf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_word2vec = save_w2v_base(files, w2v_model, mystem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "### Задание 3\n",
    "Напишите функцию обучения doc2vec на юридических текстах, и получите свою кастомную d2v модель. \n",
    "> Совет: есть мнение, что для обучения doc2vec модели не нужно удалять стоп-слова из корпуса. Они являются важными семантическими элементами.      \n",
    "\n",
    "Важно! В качестве документа для doc2vec берите **параграфы** исходного текста, а не весь текст целиком. И не забывайте про предобработку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paragraph(files_list, mystem):\n",
    "    text_ = {}\n",
    "    data = []\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            text_[file] = text\n",
    "            paragraphs = splitter(text, 1)\n",
    "            for el in paragraphs:\n",
    "                paragraph_lemmatized = preprocessing(mystem, el, del_stopwords=False)\n",
    "                data.append({'file': file, 'paragraph': paragraph_lemmatized})\n",
    "    \n",
    "    with open('file_text', 'w', encoding='utf-8') as fw:\n",
    "        json.dump(text_, fw)\n",
    "    \n",
    "    return data, text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(data, epochs):\n",
    "    tagged_data = [TaggedDocument(words=elem['paragraph'],tags=[str(i)]) for i, elem in enumerate(data)]\n",
    "    model = Doc2Vec(vector_size=100, min_count=5, alpha=0.025, min_alpha=0.025, epochs=epochs, workers=4, dm=1)\n",
    "    \n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs, file_text = paragraph(files, mystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4957\n",
      "CPU times: user 17min 59s, sys: 3min 22s, total: 21min 21s\n",
      "Wall time: 14min 45s\n"
     ]
    }
   ],
   "source": [
    "d2v_model = train_doc2vec(paragraphs, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "Напишите функцию индексации для поиска через doc2vec. Она должна для каждого документа из корпуса получать вектор.    \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только вектор, но и опознователь текста, которому он принадлежит. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_d2v_vectors(model, lemmas):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    vec = model.infer_vector(lemmas)\n",
    "    return vec\n",
    "    \n",
    "def save_d2v_base(model, paragraphs, save=True):\n",
    "    \"\"\"Индексирует всю базу для поиска через doc2vec\"\"\"\n",
    "    documents_info = []    \n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        vec = get_d2v_vectors(model, paragraph['paragraph'])\n",
    "            \n",
    "        file_info = {'file': paragraph['file'], 'doc2vec': vec}\n",
    "        documents_info.append(file_info)\n",
    "    \n",
    "    if save:\n",
    "        with open('d2v_base.pickle', 'wb') as fw:\n",
    "            pickle.dump(documents_info, fw)\n",
    "    \n",
    "    return documents_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 57s, sys: 5.59 s, total: 9min 3s\n",
      "Wall time: 9min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_doc2vec = save_d2v_base(d2v_model, paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция поиска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обратного индекса функцией поиска является Okapi BM25. Она у вас уже должна быть реализована."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция измерения близости между векторами нам пригодится:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "import numpy as np \n",
    "\n",
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "Напишите функцию для поиска через word2vec и для поиска через doc2vec, которая по входящему запросу выдает отсортированную выдачу документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_inv(query, questions, answers, inv_index) -> list:\n",
    "    \"\"\"\n",
    "    Search documents relative to query using inverted index algorithm.\n",
    "    :param query: str: input text\n",
    "    :param questions: list: all questions from corpus\n",
    "    :param answers: list: all answers from corpus\n",
    "    :param inv_index: list: questions inverted index\n",
    "    :return: list: 5 relative answers\n",
    "    \"\"\"\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    avgdl = np.mean(list(map(len, questions)))\n",
    "    N = len(questions)\n",
    "    \n",
    "    query_list = preprocessing(query)\n",
    "    scores = list()\n",
    "    \n",
    "    for i, doc in enumerate(questions):\n",
    "        score = 0\n",
    "        for word in query_list:\n",
    "            score += compute_sim(word, doc, inv_index, k1, b, avgdl, N)\n",
    "        scores.append([i, score])\n",
    "        \n",
    "    ranked = sorted(scores, key = lambda x: x[1], reverse=True)\n",
    "    result = [{'id': doc[0], 'text': answers[doc[0]]} for doc in ranked[:5]]\n",
    "\n",
    "    return result\n",
    "\n",
    "def search_w2v(query, w2v_base_quest, answers) -> list:\n",
    "    \"\"\"\n",
    "    Search documents relative to query using inverted w2v algorithm.\n",
    "    :param query: str: input text\n",
    "    :param w2v_base_quest: list: all questions' vectors from corpus\n",
    "    :param answers: list: all answers from corpus\n",
    "    :return: list: 5 relative answers\n",
    "    \"\"\"\n",
    "    \n",
    "    similarities = list()\n",
    "\n",
    "    for part in sp(query, 3):\n",
    "        lemmas = preprocessing(query)\n",
    "        vec = get_w2v_vectors(lemmas)\n",
    "    \n",
    "        for quest in w2v_base_quest:\n",
    "            s = similarity(vec, quest['vec'])\n",
    "            similarities.append({'id': quest['id'], 'sim': s})\n",
    "\n",
    "    ranked = sorted(similarities, key=lambda x: x['sim'], reverse=True)\n",
    "    result = [{'id': doc['id'], 'text': answers[doc['id']]} for doc in ranked[:5]]\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def search_d2v(query, d2v_base_quest, answers) -> list:\n",
    "    \"\"\"\n",
    "    Search documents relative to query using inverted d2v algorithm.\n",
    "    :param query: str: input text\n",
    "    :param d2v_base_quest: list: all questions' vectors from corpus\n",
    "    :param answers: list: all answers from corpus\n",
    "    :return: list: 5 relative answers\n",
    "    \"\"\"\n",
    "    similarities = list()\n",
    "\n",
    "    for part in sp(query, 3):\n",
    "        lemmas = preprocessing(query)\n",
    "        vec = get_d2v_vectors(lemmas)\n",
    "    \n",
    "        for quest in d2v_base_quest:\n",
    "            s = similarity(vec, quest['vect'])\n",
    "            similarities.append({'id': quest['id'], 'sim': s})\n",
    "\n",
    "    ranked = sorted(similarities, key=lambda x: x['sim'], reverse=True)\n",
    "    result = [{'id': doc['id'], 'text': answers[doc['id']]} for doc in ranked[:5]]   \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.2 s, sys: 12.1 s, total: 1min 7s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inverted_index, document_length = get_inverted_index(mystem, files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('inverted_index.json', 'r', encoding='utf-8') as f:\n",
    "    inverted_index = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('document_length.json', 'r', encoding='utf-8') as f:\n",
    "    document_length = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После выполнения всех этих заданий ваш поисковик готов, поздравляю!                  \n",
    "Осталось завернуть все написанное в питон скрипт, и сделать общую функцию поиска гибким, чтобы мы могли искать как по обратному индексу, так и по word2vec, так и по doc2vec.          \n",
    "Сделать это можно очень просто через старый добрый ``` if ```, который будет дергать ту или иную функцию поиска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(query, search_method, n_results=5):\n",
    "    \n",
    "    if search_method == 'inverted_index':\n",
    "        search_result = get_search_result(query, inverted_index, mystem, files_list, document_length, n_results)\n",
    "    \n",
    "    elif search_method == 'word2vec':\n",
    "        preprocessed_query = preprocessing(mystem, query)\n",
    "        search_result = search_w2v(preprocessed_query, w2v_model, data_word2vec, n_results)\n",
    "    \n",
    "    elif search_method == 'doc2vec':\n",
    "        preprocessed_query = preprocessing(mystem, query, del_stopwords=False)\n",
    "        search_result = search_d2v(preprocessed_query, d2v_model, data_doc2vec, n_results)\n",
    "    \n",
    "    else:\n",
    "        raise TypeError('unsupported search method')\n",
    "    \n",
    "    results = [(filename, file_text[filename]) for filename in search_result]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_d2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-afceed7881cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Высшего Арбитражного Суда Российской Федерации'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc2vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-9f6e52eb6715>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(query, search_method, n_results)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msearch_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'doc2vec'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpreprocessed_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmystem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msearch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_d2v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_doc2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'search_d2v' is not defined"
     ]
    }
   ],
   "source": [
    "search('Высшего Арбитражного Суда Российской Федерации', 'doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
